import os
import sys
import subprocess
import pprint

from bitextor.utils.args import validate_args
from bitextor.utils.common import (
    open_xz_or_gzip_or_plain,
    get_all_ppids,
    snake_no_more_race_get_pgid,
    duration_to_seconds,
)

valid, config = validate_args(config)

if not valid:
    raise Exception("provided configuration is not valid")

#################################################################
include: "rules/common.smk"


#################################################################
# BASIC PARAMETERS

WORKFLOW = workflow.basedir
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

LANGS = set()

# the only scenario where config won't have LANG1 and LANG2 is when the workflow is monolingual,
# and in that case LANG1/LANG2 and SRC_LANG/TRG_LANG won't be used anywhere, so setting them to 'l1' and 'l2' is fine
# (if LANG1/LANG2 and SRC_LANG/TRG_LANG are unset or both empty string, Snakemake will give an error, even if they are not used)
LANG1 = "l1"
LANG2 = "l2"

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANG1 = config["lang1"]
    LANGS.add(LANG1)
if "lang2" in config:
    LANG2 = config["lang2"]
    LANGS.add(LANG2)

PROFILING = ""
if "profiling" in config and config["profiling"]:
    PROFILING = "\\time -v"

#################################################################
# CRAWLING
CRAWLTARGET = ""
TLD_CRAWL = []
USERAGENT = "Mozilla/5.0 (compatible; Bitextor/8 +https://github.com/bitextor/bitextor)"
CRAWLSIZELIMIT = ""
CRAWLTIMELIMIT = ""
CRAWLWAIT = ""
CRAWLFILETYPES = []
CRAWLJOBS = "2"
CRAWLTIMEOUT = "10"
CRAWLDUMPARGS = ""
CONTINUECRAWL = False
HERITRIXPATH = ""
HERITRIXURL = "https://localhost:8443"
HERITRIXUSER = "admin:admin"
CRAWLMAXFOLDERTREEDEPTH = "20"
CRAWLSCOUTSTEPS = "200"
CRAWLBLACKLISTURL = ['wordpress','blogspot','facebook','google','wikipedia','youtube','perehodi','twitter','instagram']
CRAWLPREFIXFILTER = ['mailto:']

if "crawler" in config:
    CRAWLTARGET = config["crawler"]

    if CRAWLTARGET == "linguacrawl":
        # TODO should we change this default value, as we have done with wget, as well?
        CRAWLFILETYPES = ["text/html", "application/pdf"]
if "crawlTLD" in config and config["crawlTLD"]:
    TLD_CRAWL = config["crawlTLD"]
if "crawlerUserAgent" in config:
    USERAGENT = config["crawlerUserAgent"]
if "crawlSizeLimit" in config:
    CRAWLSIZELIMIT = str(config["crawlSizeLimit"])
if "crawlTimeLimit" in config:
    CRAWLTIMELIMIT = str(duration_to_seconds(config["crawlTimeLimit"]))
if "crawlWait" in config:
    CRAWLWAIT = str(config["crawlWait"])
if "crawlFileTypes" in config:
    CRAWLFILETYPES = config["crawlFileTypes"]
if "crawlerNumThreads" in config:
    CRAWLJOBS = str(config["crawlerNumThreads"])
if "crawlerConnectionTimeout" in config:
    CRAWLTIMEOUT = str(config["crawlerConnectionTimeout"])
if "dumpCurrentCrawl" in config:
    CRAWLDUMPARGS = config["dumpCurrentCrawl"]
if "resumePreviousCrawl" in config:
    CONTINUECRAWL = config["resumePreviousCrawl"]
if "heritrixPath" in config:
    HERITRIXPATH = config["heritrixPath"]
if "heritrixUrl" in config:
    HERITRIXURL = config["heritrixUrl"]
if "heritrixUser" in config:
    HERITRIXUSER = config["heritrixUser"]
if "crawlMaxFolderTreeDepth" in config:
    CRAWLMAXFOLDERTREEDEPTH = config["crawlMaxFolderTreeDepth"]
if "crawlScoutSteps" in config:
    CRAWLSCOUTSTEPS = config["crawlScoutSteps"]
if "crawlBlackListURL" in config:
    CRAWLBLACKLISTURL = config["crawlBlackListURL"]
if "crawlPrefixFilter" in config:
    CRAWLPREFIXFILTER = config["crawlPrefixFilter"]

#################################################################
# PREPROCESS
PPROC = "warc2text"
PPROC_FILES = ["text.gz", "url.gz", "mime.gz"]
TEXT_FILE = "text.gz"
HTML_FILE = ""

if "writeHTML" in config and config["writeHTML"]:
    HTML_FILE = "html.gz"
    PPROC_FILES.append("html.gz")

if "preprocessor" in config:
    if config["preprocessor"] == "warc2preprocess":
        PPROC = "w2p"
        PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
        TEXT_FILE = "plain_text.gz"
        HTML_FILE = "deboilerplate_html.gz"
    else:
        PPROC = config["preprocessor"]

SHARDS = config["shards"]
BATCHES = config["batches"]

BOILERPLATE_CLEANING = config["boilerplateCleaning"]
PARAGRAPH_IDENTIFICATION = config["paragraphIdentification"]

CLEANHTML = ""
FTFY = ""
LANGID = "cld2"
PARSER = ""
PDFEXTRACT = ""
HTML5LIB = ""

if "cleanHTML" in config and config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
if "ftfy" in config and config["ftfy"]:
    FTFY = "--ftfy"
if "langID" in config:
    LANGID = config["langID"]
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT_CF = ""
    PDFEXTRACT_SJ = ""
    PDFEXTRACT_KL = ""
    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT_CF = f" --pe_configfile {config['PDFextract_configfile']}"
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT_SJ = f" --sentence_join_path {config['PDFextract_sentence_join_path']}"
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT_KL = f" --kenlm_path {config['PDFextract_kenlm_path']}"
    PDFEXTRACT = f"--pdfextract {PDFEXTRACT_CF} {PDFEXTRACT_SJ} {PDFEXTRACT_KL}"
if "html5lib" in config and config["html5lib"]:
    HTML5LIB = "--html5lib"

# sentence splitting and tokenisation
SENTTOKS = {} if not "sentenceSplitters" in config else config["sentenceSplitters"]
CUSTOMNBPS = {} if not "customNBPs" in config else config["customNBPs"]
WORDTOKS = {} if not "wordTokenizers" in config else config["wordTokenizers"]
MORPHTOKS = {} if not "morphologicalAnalysers" in config else config["morphologicalAnalysers"]

WORDTOK1 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG1}"
WORDTOK2 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG2}"

if WORDTOKS:
    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)
    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

# Sharding
EMPTY_SHARD_CHECK = f"{TMPDIR}/empty_shard"
EMPTY_SHARD_BATCH_DIR = "EMPTY/1"

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
# mt
if DOCALIGN == "externalMT":
    MT_COMMAND = config["alignerCmd"]
else:
    MT_COMMAND = None
SRC_LANG = LANG1
TRG_LANG = LANG2
if "translationDirection" in config and config["translationDirection"] == f"{LANG2}2{LANG1}":
    SRC_LANG = LANG2
    TRG_LANG = LANG1

DOC_THRESHOLD = 0.1
if "documentAlignerThreshold" in config:
    DOC_THRESHOLD = config["documentAlignerThreshold"]

# dic-based docalign
# snakemake/include/dic-docsegalign
#################################################################
# dic
# snakemake/include/dic-generation

try:
    os.makedirs(TMPDIR)
except:
    pass

DIC = None

if "dic" in config:
    DIC = config["dic"]

#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
SEGALIGN_THRESHOLD = 0.0
if "sentenceAlignerThreshold" in config:
    SEGALIGN_THRESHOLD = config["sentenceAlignerThreshold"]
# hunalign
# snakemake/include/dic-docsegalign
#################################################################
# CLEANING
FIELDS = ["url1", "url2", "seg1", "seg2", "aligner"]
PARAGRAPH_IDENTIFICATION_FIELDS = []
DEFERRED = ""
MMHSUM_PATH = ""
DEFERRED_FIELDS = []
BIFIXER = False
BIFIXER_FIELDS = []
BIFIXER_DEFERRED_COLS = ""
AGGRESSIVE_DEDUP = "--aggressive_dedup"
BICLEANER = False
BICLEANER_MODEL = ""
BICLEANER_FIELDS = []
BICLEANER_THRESHOLD = 0.0
ELRC = False
ELRC_FIELDS = []
TMX = False
DEDUPED = False
BIROAMER = False
BIROAMER_MIX_FILES = "/dev/null"
BIROAMER_ALIGNMENT_CORPUS = ""
BIROAMER_OMIT = ""
OUTPUT_FILES = ["sent", "raw"]
STATS_FILES = ["sent", "raw"]
BICLEANER_TRAIN_PREFIXES = []

if PARAGRAPH_IDENTIFICATION:
    PARAGRAPH_IDENTIFICATION_FIELDS = ["para1", "para2"]
if "deferred" in config and config["deferred"]:
    DEFERRED = "--print-sent-hash"
    MMHSUM_PATH = f"mmhsum"
    DEFERRED_FIELDS = ["checksum1", "checksum2"]
    BIFIXER_DEFERRED_COLS = "--sdeferredcol 6 --tdeferredcol 7"

    if PARAGRAPH_IDENTIFICATION:
        BIFIXER_DEFERRED_COLS = "--sdeferredcol 8 --tdeferredcol 9"
if "bifixer" in config and config["bifixer"]:
    BIFIXER = True
    BIFIXER_FIELDS = ["bifixerhash", "bifixerscore"]
if "aggressiveDedup" in config and not config["aggressiveDedup"]:
    AGGRESSIVE_DEDUP = ""
if "bicleaner" in config and config["bicleaner"]:
    BICLEANER = True
    BICLEANER_MODEL = config["bicleanerModel"]
    BICLEANER_FIELDS = ["bicleaner"]
if "bicleanerThreshold" in config:
    BICLEANER_THRESHOLD = config["bicleanerThreshold"]
if "bicleanerCorpusTrainingPrefix" in config:
    BICLEANER_TRAIN_PREFIXES = config["bicleanerCorpusTrainingPrefix"]
if "elrc" in config and config["elrc"]:
    ELRC = True
    ELRC_FIELDS = ["lengthratio", "numTokensSL", "numTokensTL"]
if "tmx" in config and config["tmx"]:
    TMX = True
    OUTPUT_FILES.append("not-deduped.tmx")
if "deduped" in config and config["deduped"]:
    DEDUPED = True
    OUTPUT_FILES.append("deduped.tmx")
    OUTPUT_FILES.append("deduped.txt")
    STATS_FILES.append("deduped")
if "biroamer" in config and config["biroamer"]:
    BIROAMER = True

    if "deduped.tmx" in OUTPUT_FILES:
        OUTPUT_FILES.append("deduped.roamed.tmx")
    else:
        OUTPUT_FILES.append("not-deduped.roamed.tmx")

    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:
        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])

    if "biroamerImproveAlignmentCorpus" in config:
        BIROAMER_ALIGNMENT_CORPUS = f"-a {config['biroamerImproveAlignmentCorpus']}"

    if "biroamerOmitRandomSentences" in config and config["biroamerOmitRandomSentences"]:
        BIROAMER_OMIT = "-o"

BEFORE_ELRC_FIELDS = FIELDS + PARAGRAPH_IDENTIFICATION_FIELDS + DEFERRED_FIELDS + BIFIXER_FIELDS + BICLEANER_FIELDS
TMX_FIELDS = BEFORE_ELRC_FIELDS + ELRC_FIELDS

FILTER_SORT_FIELDS = "-k3,4"
TMX_DEDUP_FIELDS = "seg1,seg2"
if "bifixerhash" in BEFORE_ELRC_FIELDS:
    i = BEFORE_ELRC_FIELDS.index("bifixerhash")
    i = i + 1  # sort counts from 1, not 0
    FILTER_SORT_FIELDS = f"-k{i},{i} -k{i+1},{i+1}nr"
    TMX_DEDUP_FIELDS = "bifixerhash"

BEFORE_ELRC_FIELDS = ",".join(BEFORE_ELRC_FIELDS)
TMX_FIELDS = ",".join(TMX_FIELDS)
#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()
PREVERTICALS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])

if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])

if "preverticals" in config:
    PREVERTICALS = PREVERTICALS.union(config["preverticals"])

if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())

if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())

if "preverticalsFile" in config:
    with open_xz_or_gzip_or_plain(config["preverticalsFile"]) as f:
        for line in f:
            PREVERTICALS.add(line.strip())

# group hosts by domain and check their validity
DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)

# assign an ID to each WARC and check that all WARCs exist
TARGET_2_PROVIDED_WARCS = create_id_key_2_file_map(WARCS, file_desc="WARCs")

# assign an ID to each prevertical and check that all preverticals exist
TARGET_2_PROVIDED_PREVERTICALS = create_id_key_2_file_map(PREVERTICALS, id_offset=len(TARGET_2_PROVIDED_WARCS), file_desc="preverticals")

# group crawled WARCs by domains
TARGET_2_CRAWLED_WARCS = dict([
    (domain, [f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz" for host in hosts])
    for (domain, hosts) in DOMAIN_2_HOSTS.items()
])

# group all files (e.g. WARCs, prevertical) by target (either domain if crawled, or ID if provided by user)
TARGET_2_WARCS = {**TARGET_2_CRAWLED_WARCS, **TARGET_2_PROVIDED_WARCS}
TARGET_2_PREVERTICALS = {**TARGET_2_PROVIDED_PREVERTICALS}
#TARGETS = [*TARGET_2_WARCS.keys(), *TARGET_2_PREVERTICALS.keys()]

#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {
    "split": 1,
    "translate": 1,
    "tokenise": 1,
    "docalign": 1,
    "segalign": 1,
    "bifixer": 1,
    "bicleaner": 1,
    "sents": 1,
}
if "parallelWorkers" in config:
    for k in config["parallelWorkers"]:
        THREADS[k] = config["parallelWorkers"][k]

OUTPUT = []
UNTIL = config["until"] if "until" in config else ""
if "until" not in config:
    OUTPUT = expand(
        "{permanent}/{lang1}-{lang2}.{output_file}.gz",
        permanent=PERMANENT,
        lang1=LANG1,
        lang2=LANG2,
        output_file=OUTPUT_FILES,
    )
    OUTPUT.extend(
        expand(
            "{permanent}/{lang1}-{lang2}.stats.{stats_file}",
            permanent=PERMANENT,
            lang1=LANG1,
            lang2=LANG2,
            stats_file=STATS_FILES,
        )
    )
    # this has to be added, because tokenisation rules are the last rules that don't need both shards
    # otherwise snakemake waites for both shards be completed to continue
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")
    if DOCALIGN == "externalMT":
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    elif DOCALIGN == "DIC":
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}")
elif UNTIL == "crawl":
    if CRAWLTARGET == "linguacrawl":
        OUTPUT.append(f"{DATADIR}/warc/linguacrawl.finished")
    else:
        for domain, hosts in DOMAIN_2_HOSTS.items():
            for host in hosts:
                OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")
elif UNTIL == "preprocess":
    OUTPUT = expand(
        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_WARCS,
        pproc=PPROC,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    )
    OUTPUT.extend(expand(
        "{datadir}/preprocess/{target}/prevertical2text/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_PREVERTICALS,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    ))
elif UNTIL == "shard":
    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "split":
    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "translate":
    OUTPUT = f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}"
elif UNTIL == "tokenise_trg":
    OUTPUT = f"{DATADIR}/shards/05.tokenise.{TRG_LANG}"
elif UNTIL == "tokenise_src":
    if DOCALIGN == "externalMT":
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}"
    elif DOCALIGN == "DIC":
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}"
elif UNTIL == "tokenise":
    OUTPUT = expand("{datadir}/shards/05.tokenise.{lang}", datadir=DATADIR, lang=LANGS)
else:
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    if UNTIL == "docalign":
        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}")
    elif UNTIL == "segalign":
        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")
    elif UNTIL == "bifixer":
        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")
    elif UNTIL == "bicleaner":
        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")
    elif UNTIL == "filter":
        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")

shell.prefix("set -euo pipefail;")


#################################################################
### FINAL OUTPUTS ###############################################
rule all:
    input:
        OUTPUT,


#################################################################
### INCLUDE #####################################################
include: "rules/dict_generation.smk"
include: "rules/dict_doc_seg_align.smk"


#################################################################
### CRAWLING ####################################################

rule wget_download:
    """
    Download {target} with wget
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=apply_format(CRAWLTIMELIMIT, "-t {}s"),
        user_agent=apply_format(USERAGENT, "-a '{}'"),
        wait=apply_format(CRAWLWAIT, "--wait {}"),
        file_types=apply_format(",".join(CRAWLFILETYPES), "-f {}")
    output:
        f"{DATADIR}/warc/{{target}}/wget.warc.gz",
    shell:
        """
        mkdir -p {params.folder} {TMPDIR}
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} python3 {WORKFLOW}/bitextor_wget.py --url {params.url} --output-path $DIRNAME {params.time_limit} {params.user_agent} {params.file_types} {params.wait} --warc {output}
        rm -rf $DIRNAME
        """


rule heritrix_download:
    """
    Download {target} with heritrix
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=CRAWLTIMELIMIT
    output:
        f"{DATADIR}/warc/{{target}}/heritrix.warc.gz",
    shell:
        """
        URL=$(python3 -c "from bitextor.utils.common import check_connection; \
            e, url = check_connection('{params.url}'); \
            print(url) ; \
            exit(e)")
        if [ $? -ne 0 ]; then
            touch {DATADIR}/warc/{wildcards.target}/heritrix.warc
            gzip {DATADIR}/warc/{wildcards.target}/heritrix.warc
        else
            mkdir -p {params.folder} {TMPDIR}
            if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
                then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
            fi
            curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
            DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
            cat {WORKFLOW}/data/crawler-beans.cxml | sed "s@http://example.example/example@${{URL}}@g" > $DIRNAME/my-crawler-beans.cxml
            curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
            curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            sleep 2
            curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            RUNTIME=0
            sleep 15
            while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
            do
                sleep 5
                RUNTIME=$((RUNTIME+5))
                if [ "{params.time_limit}" != "" ]
                then
                    if [ $RUNTIME -gt "{params.time_limit}" ]
                    then
                        echo "Crawling time limit reached"
                        curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    fi
                fi
            done
            echo "Job {wildcards.target} finished!"
            cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
        fi
        """


rule linguacrawl_config:
    """
    Create a linguacrawl yaml config file from Bitextor's parameters
    """
    output:
        yaml_file=temp(f"{TMPDIR}/linguacrawl.yaml"),
    params:
        user_agent=apply_format(USERAGENT, "user_agent: '{}'\n"),
        output_dir=apply_format(f"{DATADIR}/warc/linguacrawl", "output_dir: '{}'\n"),
        wait=apply_format(CRAWLWAIT, "crawl_delay: {}\n"),
        time_limit=apply_format(CRAWLTIMELIMIT, "max_time_per_site: {}\n"),
        size_limit=apply_format(CRAWLSIZELIMIT, "max_size_per_site: {}\n"),
        timeout=apply_format(CRAWLTIMEOUT, "connection_timeout: {}\n"),
        num_threads=apply_format(CRAWLJOBS, "max_jobs: {}\n"),
        prefix_filter=apply_format(str(CRAWLPREFIXFILTER), "prefix_filter: {}\n"),
        resume_crawling=apply_format(str(CONTINUECRAWL), "resume_crawling: {}\n"),
        dump_args=apply_format(str(CRAWLDUMPARGS), "verbose: {}\n"),
        file_types=apply_format(f"({'|'.join(CRAWLFILETYPES)})", "accepted_content: '{}'\n"),
        max_folder_depth=apply_format(CRAWLMAXFOLDERTREEDEPTH, "max_folder_tree_depth: {}\n"),
        scout_steps=apply_format(CRAWLSCOUTSTEPS, "scout_steps: {}\n"),
        blacklist=apply_format(str(CRAWLBLACKLISTURL), "url_blacklist: {}\n"),
    run:
        global HOSTS
        yaml_content = ""

        langs = "','".join(LANGS)
        hosts = "','".join(HOSTS)

        # Mandatory
        yaml_content += params.user_agent
        yaml_content += f"langs_of_interest: ['{langs}']\n"
        yaml_content += params.output_dir
        # Optional (the argument parser will not complain, but the crawler will crash in some cases)
        yaml_content += params.wait
        yaml_content += params.time_limit
        yaml_content += params.size_limit
        yaml_content += params.timeout
        yaml_content += params.num_threads
        yaml_content += params.resume_crawling
        yaml_content += f"seed_urls: ['{hosts}']\n"
        # yaml_content += f"seed_urls_from_file: \n" # HOSTS contain all hosts if hosts defined either with or without file
        yaml_content += params.prefix_filter
        yaml_content += params.dump_args
        yaml_content += params.file_types
        yaml_content += params.max_folder_depth
        yaml_content += f"max_attempts: 3\n"
        yaml_content += params.scout_steps
        yaml_content += f"min_langs_in_site: 2\n"
        yaml_content += params.blacklist

        if LANG1 or LANG2:
            yaml_content += f"mandatory_lang: '{LANG1}'\n" if LANG1 else f"mandatory_lang: '{LANG2}'\n"
            yaml_content += f"min_percent_mandatory_lang: 10\n"

        # tld = LANGS.union([params.url.split('.')[-1]]).union(TLD_CRAWL)
        tld = LANGS.union([url.split(".")[-1] for url in HOSTS]).union(TLD_CRAWL)
        tld = "','".join(tld)

        yaml_content += f"accepted_tlds: ['{tld}']\n"

        fdescriptor = open(output.yaml_file, "w")
        fdescriptor.write(yaml_content)
        fdescriptor.flush()
        fdescriptor.close()


# This must be a checkpoint because we do not know the resulted warcs since we let the user decide if they want to
#  either join all the warcs (it loses important information like the source of the warcs) or not
# Manual stopping: kill -s sigint `ps aux | grep bin/linguacrawl | grep -v grep | awk '{print $2}'`
checkpoint linguacrawl_download:
    """
    Download HOSTS with linguacrawl
    """
    input:
        f"{TMPDIR}/linguacrawl.yaml"
    params:
        folder=f"{DATADIR}/warc/linguacrawl",
        crawl_cat=config["crawlCat"] if "crawlCat" in config else False,
        crawl_cat_max=config["crawlCatMaxSize"]*1024*1024 if "crawlCatMaxSize" in config else 0
    output:
        f"{DATADIR}/warc/linguacrawl.finished",
    run:
        shell(
            """
            mkdir -p {params.folder} {TMPDIR}
            {PROFILING} linguacrawl {input}
            """
        )

        all_files = subprocess.Popen(("ls", params.folder), stdout=subprocess.PIPE)
        try:
            # Process the resulted warcs
            warcs = subprocess.check_output(
                ("grep", "[.]warc[.]gz$"), stdin=all_files.stdout
            )  # It will throw an exception if no warcs were downloaded
            all_files.wait()

            warcs = warcs.decode("utf-8").split("\n")
            warcs = list(filter(lambda warc: len(warc) != 0, warcs))
            warcs = list(map(lambda warc: f"{params.folder}/{warc}", warcs))

            if not params.crawl_cat:
                warcs = "\n".join(warcs)
                shell(f'echo -e "{warcs}" > {{output[0]}}')
            else:
                if params.crawl_cat_max <= 0:
                    warcs = " ".join(warcs)
                    shell(
                        f"""
                        cat {warcs} > {params.folder}/linguacrawl.warc.gz
                        echo "{params.folder}/linguacrawl.warc.gz" > {{output[0]}}
                        """
                    )
                else:
                    # Improve the number of preprocess rules that are executed
                    current = 0
                    blocks = 0
                    files = [f"{params.folder}/linguacrawl{blocks}.warc.gz"]

                    for warc in warcs:
                        if current > {params.crawl_cat_max}:
                            current = 0
                            blocks += 1
                            files.append(f"{params.folder}/linguacrawl{blocks}.warc.gz")

                        shell(f"cat {warc} >> {files[-1]}")

                        du_command = subprocess.Popen(("du", "-b", warc), stdout=subprocess.PIPE)

                        try:
                            du_warc = subprocess.check_output(("awk", "{print $1}"), stdin=du_command.stdout)
                            du_command.wait()

                            current += int(du_warc)
                        except:
                            sys.stderr.write(f"WARNING: could not retrieve the size of {warc}")

                    files = "\n".join(files)
                    shell(f'echo -e "{files}" > {{output[0]}}')
        except:
            # No warcs were downloaded: error or warning
            if len(WARCS) == 0:
                sys.stderr.write("ERROR: could not find any file after crawling\n")
                sys.exit(1)
            sys.stderr.write("WARNING: could not find any file after crawling\n")
            shell(f"touch {output}")


#################################################################
### PREPROCESS ##################################################
rule warc2preprocess:
    """
    Process a list of WARCs (or a single WARC)
        and produce {plain_text,mime,url,normalized_html,deboilerplate_html}.gz
    """
    input:
        get_pproc_input,
    output:
        expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES),
    threads: 2
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        pproclangs=",".join(LANGS),
        boilerplate='--boilerpipe' if BOILERPLATE_CLEANING else '',
        paragraphs='--paragraph-identification' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        cat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip \
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2preprocess.py --input - --langs {params.pproclangs} \
                --compression gz --langid {LANGID} {params.boilerplate} {HTML5LIB} {PARSER} {params.paragraphs} \
                --output-dir {params.folder}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule warc2text:
    """
    Process a list of WARCs (or a single WARC)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        get_pproc_input,
    output:
        expand(
            "{data}/preprocess/{{target}}/warc2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
    shell:
        """
        mkdir -p {params.folder}
        {PROFILING} warc2text -o {params.folder} -s -f {params.f} {input}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule prevertical2text:
    """
    Process a list of prevertical format files (or a single prevertical format file)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        lambda wildcards: TARGET_2_PREVERTICALS[wildcards.target],
    output:
        expand(
            "{data}/preprocess/{{target}}/prevertical2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        boilerplate='-b' if BOILERPLATE_CLEANING else '',
        paragraphs='-p' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        cat {input} | \
            python3 {WORKFLOW}/bitextor_prevertical_lang_iso639_1.py | \
            {PROFILING} prevertical2text -o {params.folder} -s -f {params.f} {params.boilerplate} {params.paragraphs} -
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    """
    Use giashard to shard the output of warc2text/warc2preprocess to balance jobs
    :input: output of the preprocessing rules
    :output: a plain text files that contains the list of every batch generated
        (i.e. each line has a path to a batch folder)
    """
    input:
        # this is separated in two functions to make sure that
        # the preprocessing of the provided files and the crawling
        # may be executed in parallel
        # (otherwise the linguacrawl checkpoint might prevent that)
        get_shard_input_files,
        get_shard_input_crawled,
    output:
        f"{DATADIR}/shards/02.batches.{{lang}}",  # list of batches created for lang
    params:
        n=SHARDS,
        b=BATCHES,
        o_no_lang=lambda wildcards, output: os.path.dirname(output[0]),
        o=f"{DATADIR}/shards/{{lang}}",
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
    shell:
        """
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run

        binary=giashard
        if [[ "$(command -v $binary)" == "" ]]; then
            binary=~/go/bin/giashard
        fi

        warcs=$(echo {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang})
        preverticals=$(echo {DATADIR}/preprocess/*/prevertical2text/{wildcards.lang})

        if [[ "$(echo $warcs | grep \*)" != "" ]]; then
            warcs=""
        fi
        if [[ "$(echo $preverticals | grep \*)" != "" ]]; then
            preverticals=""
        fi

        {PROFILING} $binary -n {params.n} -b {params.b} -o {params.o} -f {params.f} $preverticals $warcs

        nofiles=$(ls {params.o} | wc -l)

        if [[ "$nofiles" == "0" ]] && [[ -f "{EMPTY_SHARD_CHECK}_{wildcards.lang}" ]]; then
            # No files generated
            >&2 echo "WARNING: no files generated after running giashard for lang '{wildcards.lang}': creating empty files instead"

            # Generate empty shards if needed in order to avoid the pipeline to break
            mkdir -p {params.o}/{EMPTY_SHARD_BATCH_DIR}

            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/empty
            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
            gzip {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
        fi

        ls -d {params.o}/*/* > {output}
        """


rule split:
    """
    Use sentence splitter to obtain sentences from the plain text file
    :input: gz-compressed file with a base64-encoded document per line
        document is the plain text extracted by the preprocess
    :output: gz-compressed file with a base64-encoded document per line
        output must have the same number of lines as the input (i.e. same number of docs)
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/{TEXT_FILE}",
    params:
        splitter=lambda wildcards: apply_format(get_lang_or_default(SENTTOKS, wildcards.lang), '--sentence-splitter "{}"'),
        customnbp=lambda wildcards: apply_format(get_customnbp(CUSTOMNBPS, wildcards.lang), '--customnbp "{}"'),
        paragraphs='--process-paragraphs' if PARAGRAPH_IDENTIFICATION else '',
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz",
    threads: THREADS["split"]
    shell:
        """
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_split.py \
                {params.splitter} {params.customnbp} \
                --langcode "{wildcards.lang}" \
                {PRUNE_THRESHOLD} {PRUNE_TYPE} {params.paragraphs} \
            | pigz -c > {output}
        """


rule aggregate_split:
    """
    Helper rule to implement until=split config
    :input: the result of every split rule
    :output: a file that contains the path to the output of every split rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/03.split.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


#################################################################
### DOCALIGN ####################################################
rule aggregate_matches:
    """
    Helper rule to implement until=docalign config
    :input: the result of every docalign result
    :output: a file that contains the path to the output of every matches file
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_01.matches"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# MT ############################################################
rule custom_translate:
    """
    Translate source documents into target language (according to translationDirection)
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded translated document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each translated document must have the same number of lines as the source
    """
    input:
        source=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    threads: max(2, THREADS["translate"])
    params:
        THREADS["translate"],
    shell:
        """
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        zcat {input.source} \
            | {PROFILING} b64filter cache ${{parallel_cmd}} {MT_COMMAND} \
            | pigz -c > {output}
        n_before=$(zcat {input.source} | base64 -d | wc -l)
        n_after=$(zcat {output} | base64 -d | wc -l)
        # echo "Check lines count: $n_before -> $n_after for {SRC_LANG}/{wildcards.shard}/{wildcards.src_batch}"
        if [ $n_before -ne $n_after ]
        then
            >&2 echo "Lines count differ: source $n_before, target $n_after"
            exit 1
        fi
        """


rule aggregate_translate:
    """
    Helper rule to implement until=translate config
    :input: the result of every translate rule
    :output: a file that contains the path to the output of every translate rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule tokenise_translated:
    """
    Tokenise the output of translation rule to feed into mt-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of translation
    :output: gz-compressed file with a base64-encoded translated and tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        rules.custom_translate.output,
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    shell:
        """
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        """


rule tokenise:
    """
    Tokenise the target side documents to feed into mt-docalign or dic-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/tokenised.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, wildcards.lang), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, wildcards.lang), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    shell:
        """
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {wildcards.lang} \
            | pigz -c > {output}
        """

rule aggregate_tokenise:
    """
    Helper rule to implement until=tokenise_trg config
    :input: the result of every tokenise rule
    :output: a file that contains the path to the output of every tokenise rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/05.tokenise.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule aggregate_translate_tokenise_source:
    """
    Helper rule to implement until=tokenise_src config
    :input: the result of every tokenise_translated rule
    :output: a file that contains the path to the output of every tokenise_translated rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule mt_matches:
    """
    Align documents using mt document aligner
    :input.l1: gz-compressed file with a base64-encoded document per line
        source language document translated into target language document, and tokenised
    :input.l2: gz-compressed file with a base64-encoded document per line
        tokenised target language documents
    :output: indices file
        plain text file with 3 tab separated columns: score, src_index, trg_index
            src_index is the number of the aligned document in source language
            trg_index is the number of the aligned document in target language
    """
    input:
        l1=rules.tokenise_translated.output,
        l2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz",
    output:
        f"{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_01.matches",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
    threads: THREADS["docalign"]
    shell:
        """
        mkdir -p {params.folder}
        {PROFILING} docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} -j {threads} > {output}
        """


### SEGALIGN ####################################################
rule aggregate_segalign:
    """
    Helper rule to implement until=segalign config
    :input: the result of every segalin result
    :output: a file that contains the path to every segalign result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_02.segalign.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# BLEUALIGN #####################################################
rule bleualign:
    """
    Use bleualign to align sentences withing the matched documents
    :input.indices: output of mt docalign (columns are "score src_index trg_index")
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :input.url1: gz-compressed file with a URL per line for source (source of the corresponding documents)
    :input.url2: gz-compressed file with a URL per line for target (source of the corresponding documents)
    :input.translated1: gz-compressed file with a base64-encoded document per line
        source documents translated into target language
    :output: aligned sentences
        gz-compressed file with 5 tab-separated columns: url1,url2,sentence1,sentence2,score
    """
    input:
        indices=rules.mt_matches.output,
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
        translated1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        workers=THREADS["segalign"],
        paragraphs='--paragraph-identification' if PARAGRAPH_IDENTIFICATION else '',
    # in segalign rule output columns are reordered (or not) in accordance with translationDirection
    output:
        f"{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_02.segalign.gz",
    threads: max(THREADS["segalign"], 2)
    shell:
        """
        mkdir -p {params.folder}
        parallel_cmd=""
        if [ {params.workers} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params.workers} -l 1 --group"
        fi
        cut -f 2,3 {input.indices} \
            | docjoin \
                -l {input.url1} -r {input.url2} \
                -l {input.plain1} -r {input.plain2} \
                -l {input.translated1} \
            | {PROFILING} ${{parallel_cmd}} bleualign_cpp {DEFERRED} {params.paragraphs} \
                --bleu-threshold {SEGALIGN_THRESHOLD} \
            | pigz -c > {output}
        """


### FILTERING AND CLEANING ######################################

split_input_filename = "06_02.segalign"
if SEGALIGN == "hunalign":
    split_input_filename = "hunalign.06_02.segalign"


# split segalign results into balanced chunks
checkpoint split_segalign:
    """
    Join all of the output of segalign and split them again into even chunks this time
        each batch is a gz-compressed file that has the same format as segalign output
        if translatationDirection is different from lang1->lang2, in this rule the columns are switched
    :input: the result of every segalin result
    :output: a plain text files that contains the list of every postprocessing batch generated
        (i.e. each line has a path to a postprocessing batch file without the extension)
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{split_input_filename}.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        batches=f"{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches",
    params:
        size=BATCHES,  # use same parameter as for shards
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}",
    run:
        # We need to make this check in order to avoid the pipeline to break in the case that no input was received (e.g. no common shards)
        # It is needed to run this piece of code directly in Python and not in bash because Snakemake attempts to replace {input[0]} before
        #  running the script, so it fails even before we are able to check if the input is empty in bash script
        if len(input) == 0:
            shell(
                f"""
                >&2 echo "INFO: no data to work with: stopping execution"
                touch {output}
                # Kill only the current Snakemake, not all of them (might be running multiple instances, and we only want to stop this one)
                # https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-do-i-exit-a-running-snakemake-workflow
                pid_to_kill="{snake_no_more_race_get_pgid()}"
                if [[ "$pid_to_kill" == "" ]]; then
                    echo "ERROR: could not stop the execution with a normal status: forcing snakemake to stop"
                    exit 1
                else
                    kill -TERM "$pid_to_kill" || exit 1
                fi
                """
            )
        else:
            shell(
                """
                mkdir -p {params.folder}
                rm -f {params.folder}/* # remove anything that might be left after a previous run
                CAT=cat
                if [[ {input[0]} == *.gz ]]; then
                    CAT=zcat
                fi
                format='$2,$1,$4,$3,$5'
                if [[ "{PARAGRAPH_IDENTIFICATION}" == "True" ]] && [[ ! -z "{DEFERRED}" ]]; then
                    format='$2,$1,$4,$3,$5,$7,$6,$9,$8'
                elif [[ "{PARAGRAPH_IDENTIFICATION}" == "True" ]] || [[ ! -z "{DEFERRED}" ]]; then
                    format='$2,$1,$4,$3,$5,$7,$6'
                fi
                $CAT {input} \
                    | ( [ "{SRC_LANG}" = "{LANG1}" ] && cat || awk -F '\t' '{{ print '$format' }}' OFS='\t' ) \
                    | python3 {WORKFLOW}/bitextor_split_segalign.py -f 3,4 -s {params.size} --gzip -o "{params.folder}/"
                if [ -z "$(ls -A {params.folder})" ]; then
                    cat < /dev/null > {output.batches}
                else
                    ls {params.folder}/* | sed 's/.gz$//g' > {output.batches}
                fi
                """
            )


rule bifixer:
    """
    Apply bifixer to the segalign output
    :input: a single chunk of segalign output
        gz-compressed, columns are: url1 url2 sent1 sent2 score deferred1 deferred2
        (deferred is optional)
    :output: plain text, marked as temp, same columns as input with two new columns: hash and score
    """
    input:
        segalign=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz",
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}"),
    threads: THREADS["bifixer"]
    shell:
        """
        CAT=cat; if [[ {input.segalign} == *.gz ]]; then CAT=zcat; fi
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        $CAT {input.segalign} \
            | {PROFILING} ${{parallel_cmd}} bifixer -q - - {LANG1} {LANG2} {AGGRESSIVE_DEDUP} {BIFIXER_DEFERRED_COLS} \
            > {output}
        """


rule aggregate_bifixer:
    """
    Helper rule to implement until=bifixer config
    :input: the result of every bifixer result
    :output: a file that contains the path to every bifixer result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


bicleaner_input = rules.bifixer.output
if not BIFIXER:
    bicleaner_input = rules.bifixer.input.segalign


rule bicleaner_train_model:
    """
    Train bicleaner model
    """
    input:
        corpusl1=expand("{dataset}.{lang}.xz", dataset=BICLEANER_TRAIN_PREFIXES, lang=LANG1),
        corpusl2=expand("{dataset}.{lang}.xz", dataset=BICLEANER_TRAIN_PREFIXES, lang=LANG2),
        e2f=f"{DIC}.lex.e2f.gz",
        f2e=f"{DIC}.lex.f2e.gz",
        vcb1=f"{mgizaModelDir}/corpus.{LANG1}.filtered.vcb.gz",
        vcb2=f"{mgizaModelDir}/corpus.{LANG2}.filtered.vcb.gz",
    output:
        model=BICLEANER_MODEL,
    shell:
        """
        training=$(mktemp {TMPDIR}/train.XXXXXXXX)
        paste <(xzcat -f {input.corpusl1}) <(xzcat -f {input.corpusl2}) > $training
        DIR=$(dirname {BICLEANER_MODEL})
        echo $DIR
        lines=$(cat $training | wc -l)
        trainlines=$(echo \"$lines*4/10\" | bc)
        testlines=$(echo \"($lines-2*$trainlines)/2\" | bc)
        {PROFILING} bicleaner-train $training -S "{WORDTOK1}" -T "{WORDTOK2}" --treat_oovs --normalize_by_length -s {LANG1} -t {LANG2} -d {input.e2f} -D {input.f2e} -f {input.vcb1} -F {input.vcb2} -c $DIR/{LANG1}-{LANG2}.classifier -m {BICLEANER_MODEL} --classifier_type random_forest
        rm $training
        """


rule bicleaner:
    """
    Compute bicleaner scores of the aligned sentence pairs
    :input.bifixer: either the output of bifixer rule, or a single chunk of segalign output if bifixer is disabled
    :input.model: bicleaner model, either provided by the user or generated by train_bicleaer
    :output: gz-compressed, same columns as input with one new column: score
    """
    input:
        bifixer=bicleaner_input,
        model=BICLEANER_MODEL,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz",
    params:
        THREADS["bicleaner"],
    threads: max(2, THREADS["bicleaner"])
    shell:
        """
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        CAT=cat; if [[ {input.bifixer} == *.gz ]]; then CAT=zcat; fi
        slang=$(egrep "source_lang" {input.model} | cut -d " " -f 2)
        if [ "$slang" == "{LANG1}" ]; then
            $CAT {input.bifixer} \
                | {PROFILING} cache -k 3,4 ${{parallel_cmd}} bicleaner-classify-lite -S "{WORDTOK1}" -T "{WORDTOK2}" --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        else
            $CAT {input.bifixer} \
                | awk ' BEGIN {{FS="\t"; OFS="\t"}} {{ t = $3; $3 = $4; $4 = t; print;}} ' \
                | {PROFILING} cache -k 3,4 ${{parallel_cmd}} bicleaner-classify-lite -S "{WORDTOK1}" -T "{WORDTOK2}" --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        fi
        """


rule aggregate_bicleaner:
    """
    Helper rule to implement until=bicleaner config
    :input: the result of every biclenaer result
    :output: a file that contains the path to every biclenaer result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


filter_input = rules.bicleaner.output
if not BICLEANER:
    filter_input = rules.bicleaner.input.bifixer


rule filter:
    """
    Filter by biclenaer threshold (if applicable), compute ELRC metrics (if applicable), sort by sentence pair or bifixer hash
    :input: either the output of bicleaner (if enabled), or the output of the previous step (i.e. what would be the input of bicleaner if it was enabled)
    :output: plain-text file, marked as temp, the senteces are sorted by duplicates
        remove sentences below biclenaer threshold (if applicable)
        add new columns for ELRC if applicable: length_ration, num_tokens_src, num_tokens_trg
    """
    input:
        filter_input,
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}"),
    threads: lambda wildcards: 2 if BICLEANER and ELRC else 1
    run:
        cat_cmd = "cat"
        if input[0][-3:] == ".gz":
            cat_cmd = "zcat"
        cmd = f""" {cat_cmd} {input} """
        if BICLEANER:
            cmd += (
                f""" | {PROFILING} python3 {WORKFLOW}/bitextor_filterbicleaner.py --threshold {BICLEANER_THRESHOLD} """
            )
        if ELRC:
            cmd += f""" | {PROFILING} python3 {WORKFLOW}/bitextor_elrc_filtering.py -c "{BEFORE_ELRC_FIELDS}" -s """
        cmd += f""" | LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} """  # sorted by either sentences or bifixer
        cmd += f""" > {output} """
        shell(cmd)


rule aggregate_filter:
    """
    Helper rule to implement until=filter config
    :input: the result of every filter result
    :output: a file that contains the path to every filter result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


raw_input_filename = rules.filter.input[0].split("/")[-2]  # 06_02.segalign / 07_01.bifixer / 07_02.bicleaner
extension = ""
if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]:
    extension = ".gz"


rule raw:
    """
    Create {lang1}-{lang2}.raw.gz file by concatenating the output chunks of the last step before filtering
        may be segalign, bifixer or biclenaer, depending on what's enabled in the config
    :input: the output of the last step
    :output.corpus: the concatenated inputs, columns are the same as the input
    :output:stats: the corresponding stats file in plain text
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}{extension}" for batch in get_postproc_batches()
        ],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.raw.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.raw",
    shell:
        """
        if [[ "{extension}" == ".gz" ]]; then
            cat {input} > {output.corpus}
        else
            cat {input} | pigz -c > {output.corpus}
        fi
        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.corpus} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        """


rule sents:
    """
    Create {lang1}-{lang2}.sent.gz by concatenated and merge-sorting the outputs of filters rule
    :input: the outputs of filter step
    :output: the concatenated inputs, sorted, same columns as input
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.sent.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.sent",
    threads: THREADS["sents"]
    shell:
        """
        LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} --parallel {threads} --compress-program=gzip -T {TMPDIR} --merge {input} \
            | pigz -c > {output.corpus}
        echo "{LANG1}-{LANG2} filtered" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.corpus} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        """


rule tmx:
    """
    Crate {lang1}-{lang2}.not-deduped.tmx.gz output
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output: corpus after filtering in TMX format
    """
    input:
        rules.sents.output.corpus,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz",
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" \
            | pigz -c > {output}
        """


rule deduped_tmx:
    """
    Create deduped corpus, TMX and txt versions
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output.tmx: TMX deduplicated corpus
    :output.txt: txt dedpulicated corpus
    :output.stats: the corresponding stats file (calculated from dedup txt version)
    """
    input:
        rules.sents.output.corpus,
    output:
        tmx=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz",
        txt=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.deduped",
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" --dedup "{TMX_DEDUP_FIELDS}" -f {output.txt} \
            | pigz -c > {output.tmx}
        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.txt} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        """


roam_tmx_output = f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped.roamed.tmx.gz"
roam_tmx_input = rules.tmx.output
roam_txt_input = rules.sents.output.corpus

if DEDUPED:
    roam_tmx_output = f"{PERMANENT}/{LANG1}-{LANG2}.deduped.roamed.tmx.gz"
    roam_tmx_input = rules.deduped_tmx.output.tmx
    roam_txt_input = rules.deduped_tmx.output.txt


rule roam_tmx:
    """
    Use biroamer to created ROAMED version of the corpus
    :input.tmx: deduped.tmx corpus if dedup is active, not-deduped.tmx corpus if only tmx=true and dedup=false
    :input.txt: deduped.txt corpus if dedup is active, sents corpus if tmx=true and dedup=false
    :output: deduped.roamed.tmx corpus if dedup is active, not-deduped.roamed.tmx corpus if tmx=true and dedup=false
    """
    input:
        tmx=roam_tmx_input,
        txt=roam_txt_input,
    output:
        roam_tmx_output,
    params:
        tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG1), '-t "{}"'),
        tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG2), '-T "{}"'),
    shell:
        """
        mix_files="$(mktemp {TMPDIR}/roam_tmx_mix.{LANG1}-{LANG2}.XXXXXX)"
        cat {BIROAMER_MIX_FILES} > $mix_files
        nolines=$(cat $mix_files | wc -l)

        if [[ "$nolines" != "0" ]]; then
            mix_files="-m $mix_files"
        else
            mix_files=""
        fi

        CAT=cat; if [[ {input.txt} == *.gz ]]; then CAT=zcat; fi
        nolines_txt=$($CAT {input.txt} | wc -l)
        total_nolines=$(echo $nolines + $nolines_txt | bc)

        if [[ "$total_nolines" -lt "100000" ]] && [[ "{BIROAMER_ALIGNMENT_CORPUS}" == "" ]]; then
            >&2 echo "WARNING: biroamer suggests, at least, 100k lines in order to have a good alignment (current nolines: $total_nolines)."\
                     "Check 'biroamerImproveAlignmentCorpus' config option if you want to improve the resulted roamed tmx"
        fi

        zcat {input.tmx} \
            | {PROFILING} biroamer {params.tokenizer_l1} {params.tokenizer_l2} \
                {BIROAMER_OMIT} {BIROAMER_ALIGNMENT_CORPUS} $mix_files {LANG1} {LANG2} \
            | pigz -c > {output}
        """
